"""
Async Omni Engine for vLLM-Omni V1 architecture.

Manages multiple stages with StageAsyncCoreClient.
The Orchestrator runs as a coroutine in a background thread (with its own
asyncio event loop).  AsyncOmniEngine in the caller's thread is a thin
proxy that communicates with the Orchestrator via asyncio.Queues.
"""

from __future__ import annotations

import asyncio
import concurrent.futures
import threading
import time as _time
from dataclasses import dataclass, field
from typing import Any

import torch
from omegaconf import OmegaConf
from vllm.config import ModelConfig, VllmConfig
from vllm.inputs import PromptType
from vllm.logger import init_logger
from vllm.lora.request import LoRARequest
from vllm.pooling_params import PoolingParams
from vllm.sampling_params import SamplingParams
from vllm.tokenizers import TokenizerLike, cached_tokenizer_from_config
from vllm.v1.engine import EngineCoreRequest

from vllm_omni.distributed.omni_connectors import initialize_orchestrator_connectors
from vllm_omni.engine import (
    AdditionalInformationEntry,
    AdditionalInformationPayload,
    OmniEngineCoreRequest,
    PromptEmbedsPayload,
)
from vllm_omni.engine.input_processor import OmniInputProcessor
from vllm_omni.engine.output_processor import MultimodalOutputProcessor
from vllm_omni.engine.stage_async_core_client import StageAsyncCoreClient
from vllm_omni.engine.stage_init import (
    acquire_device_locks,
    build_vllm_config,
    extract_stage_metadata,
    prepare_engine_environment,
    release_device_locks,
    setup_stage_devices,
)
from vllm_omni.entrypoints.utils import (
    load_stage_configs_from_model,
    load_stage_configs_from_yaml,
    resolve_model_config_path,
)

logger = init_logger(__name__)


def _inject_global_id(target: Any, request_id: str) -> None:
    """Inject global_request_id into a prompt dict's additional_information."""
    if isinstance(target, dict):
        if "additional_information" not in target:
            target["additional_information"] = {}
        if target["additional_information"] is None:
            target["additional_information"] = {}
        if isinstance(target["additional_information"], dict):
            target["additional_information"][
                "global_request_id"
            ] = [str(request_id)]


def _build_engine_core_request_from_tokens(
    request_id: str,
    prompt: dict[str, Any],
    params: SamplingParams | PoolingParams,
    arrival_time: float | None = None,
    model_config: ModelConfig | None = None,
) -> OmniEngineCoreRequest:
    """Build an OmniEngineCoreRequest directly from an OmniTokensPrompt.

    Lightweight alternative to the full InputProcessor pipeline — skips
    tokenization, multimodal preprocessing, LoRA validation, and platform
    validation.  Intended for stage 1+ where the upstream stage has already
    produced token IDs and optional embeddings.
    """
    if arrival_time is None:
        arrival_time = _time.time()

    prompt_token_ids = prompt["prompt_token_ids"]

    # Clone params and set max_tokens if needed
    sampling_params = None
    pooling_params = None
    if isinstance(params, SamplingParams):
        sampling_params = params.clone()
        if sampling_params.max_tokens is None and model_config is not None:
            sampling_params.max_tokens = (
                model_config.max_model_len - len(prompt_token_ids)
            )
    else:
        pooling_params = params.clone()

    # TODO: payload are parsed with the request by now. But will move to connector later.
    # Serialize prompt_embeds if present
    prompt_embeds_payload: PromptEmbedsPayload | None = None
    pe: torch.Tensor | None = prompt.get("prompt_embeds")
    if pe is not None:
        pe_cpu = pe.detach().to("cpu").contiguous()
        prompt_embeds_payload = PromptEmbedsPayload(
            data=pe_cpu.numpy().tobytes(),
            shape=list(pe_cpu.shape),
            dtype=OmniInputProcessor._dtype_to_name(pe_cpu.dtype),
        )

    # Serialize additional_information if present
    additional_info_payload: AdditionalInformationPayload | None = None
    raw_info: dict[str, Any] | None = prompt.get("additional_information")
    if raw_info is not None:
        entries: dict[str, AdditionalInformationEntry] = {}
        for key, value in raw_info.items():
            if isinstance(value, torch.Tensor):
                v_cpu = value.detach().to("cpu").contiguous()
                entries[key] = AdditionalInformationEntry(
                    tensor_data=v_cpu.numpy().tobytes(),
                    tensor_shape=list(v_cpu.shape),
                    tensor_dtype=OmniInputProcessor._dtype_to_name(v_cpu.dtype),
                )
            elif isinstance(value, list):
                entries[key] = AdditionalInformationEntry(list_data=value)
            else:
                logger.warning(
                    "[_build_engine_core_request_from_tokens] req=%s "
                    "skipping unsupported type key=%s type=%s",
                    request_id, key, type(value).__name__,
                )
        additional_info_payload = AdditionalInformationPayload(entries=entries)

    eos_token_id = None
    if model_config is not None and hasattr(model_config, "hf_config"):
        eos_token_id = getattr(model_config.hf_config, "eos_token_id", None)

    return OmniEngineCoreRequest(
        request_id=request_id,
        prompt_token_ids=prompt_token_ids,
        mm_features=None,
        sampling_params=sampling_params,
        pooling_params=pooling_params,
        eos_token_id=eos_token_id,
        arrival_time=arrival_time,
        lora_request=None,
        cache_salt=None,
        data_parallel_rank=None,
        prompt_embeds=prompt_embeds_payload,
        additional_information=additional_info_payload,
    )


# ============================================================
# Orchestrator internals (run inside the background thread)
# ============================================================

@dataclass
class _OrchestratorRequestState:
    """Per-request bookkeeping inside the Orchestrator."""

    request_id: str
    prompt: Any = None
    sampling_params_list: list[Any] = field(default_factory=list)
    final_stage_id: int = 0


class _Orchestrator:
    """Runs inside a background thread's asyncio event loop.

    Owns all StageAsyncCoreClient instances, input/output processors,
    and handles stage-to-stage transfer logic.
    """

    def __init__(
        self,
        model: str,
        stage_configs: Any,
        request_queue: asyncio.Queue,
        output_queue: asyncio.Queue,
        stage_init_timeout: int = 300,
        log_requests: bool = True,
    ) -> None:
        self.model = model
        self.stage_configs = stage_configs
        self.request_queue = request_queue
        self.output_queue = output_queue
        self.stage_init_timeout = stage_init_timeout
        self.log_requests = log_requests

        self.num_stages = len(stage_configs)

        # Will be populated by _initialize_stages
        self.stage_clients: list[StageAsyncCoreClient] = []
        self.output_processors: list[MultimodalOutputProcessor] = []
        self.stage_tokenizers: list[TokenizerLike] = []
        self.stage_vllm_configs: list[VllmConfig] = []
        self.connectors: dict[tuple[str, str], Any] = {}
        self._stage0_input_processor: OmniInputProcessor | None = None

        # Per-request state
        self.request_states: dict[str, _OrchestratorRequestState] = {}

        # Shutdown coordination
        self._shutdown_event = asyncio.Event()

        self._initialize_stages()

    def _initialize_stages(self) -> None:
        """Initialize all stage clients and their processors sequentially.

        Performs one-time environment setup, initializes connectors,
        then for each stage: extracts metadata, sets up devices, builds
        vllm config, acquires device locks, creates the client, and
        builds tokenizer / input / output processors.
        """
        from vllm_omni.entrypoints.stage_utils import _to_dict

        # One-time global setup
        prepare_engine_environment()

        # Initialize connectors once
        try:
            config_path = resolve_model_config_path(self.model)
            omni_transfer_config, connectors = initialize_orchestrator_connectors(
                config_path,
                worker_backend="multi_process",
                shm_threshold_bytes=65536,
            )
            self.omni_transfer_config = omni_transfer_config
            self.connectors = connectors
            if connectors:
                logger.info(
                    f"[Orchestrator] Initialized {len(connectors)} connectors"
                )
        except Exception as e:
            logger.warning(f"[Orchestrator] Failed to initialize connectors: {e}")
            self.omni_transfer_config = None
            self.connectors = {}

        # Per-stage initialization
        for stage_id, stage_cfg in enumerate(self.stage_configs):
            logger.info(f"[Orchestrator] Initializing stage {stage_id}")

            metadata = extract_stage_metadata(stage_cfg)
            setup_stage_devices(stage_id, metadata.runtime_cfg)
            vllm_config, executor_class = build_vllm_config(stage_cfg, self.model)

            engine_args_dict = _to_dict(stage_cfg.engine_args)
            engine_args_dict["model"] = self.model
            lock_fds = acquire_device_locks(
                stage_id, engine_args_dict, self.stage_init_timeout
            )

            try:
                stage_client = StageAsyncCoreClient(
                    vllm_config=vllm_config,
                    executor_class=executor_class,
                    metadata=metadata,
                )
            finally:
                release_device_locks(lock_fds)

            if vllm_config.model_config.skip_tokenizer_init:
                tokenizer = None
            else:
                tokenizer = cached_tokenizer_from_config(
                    model_config=vllm_config.model_config
                )

            if stage_id == 0:
                self._stage0_input_processor = OmniInputProcessor(
                    vllm_config=vllm_config,
                    tokenizer=tokenizer,
                )

            stage_output_processor = MultimodalOutputProcessor(
                tokenizer=tokenizer,
                log_stats=False,
                engine_core_output_type=metadata.engine_output_type,
            )

            self.stage_clients.append(stage_client)
            self.output_processors.append(stage_output_processor)
            self.stage_tokenizers.append(tokenizer)
            self.stage_vllm_configs.append(vllm_config)

            logger.info(f"[Orchestrator] Stage {stage_id} initialized")

    async def run(self) -> None:
        """Main entry point for the Orchestrator event loop."""
        # Collect metadata to send back to the main thread
        default_sampling_params_list = [
            sc.default_sampling_params for sc in self.stage_clients
        ]
        stage_metadata = []
        for sc in self.stage_clients:
            stage_metadata.append({
                "final_output": sc.final_output,
                "final_output_type": sc.final_output_type,
                "stage_type": sc.stage_type,
            })

        await self.output_queue.put({
            "type": "ready",
            "num_stages": self.num_stages,
            "default_sampling_params_list": default_sampling_params_list,
            "stage_metadata": stage_metadata,
            "input_processor": self._stage0_input_processor,
            "output_processors": self.output_processors,
        })

        logger.info("[Orchestrator] Ready signal sent, starting event loop")

        request_task = asyncio.create_task(
            self._request_handler(), name="orchestrator-request-handler"
        )
        output_task = asyncio.create_task(
            self._output_handler(), name="orchestrator-output-handler"
        )

        try:
            # _request_handler exits on shutdown; once it's done, cancel
            # the output handler so we don't leave dangling wait_for tasks.
            await request_task
        finally:
            output_task.cancel()
            try:
                await output_task
            except asyncio.CancelledError:
                pass

            # Cancel any remaining tasks spawned by wait_for / gather so
            # the event loop can close cleanly without "pending task" errors.
            loop = asyncio.get_running_loop()
            pending = [
                t for t in asyncio.all_tasks(loop)
                if t is not asyncio.current_task() and not t.done()
            ]
            for t in pending:
                t.cancel()
            if pending:
                await asyncio.gather(*pending, return_exceptions=True)

    async def _request_handler(self) -> None:
        """Read messages from the main thread via request_queue."""
        while True:
            msg = await self.request_queue.get()
            msg_type = msg.get("type")

            if msg_type == "add_request":
                await self._handle_add_request(msg)
            elif msg_type == "abort":
                await self._handle_abort(msg)
            elif msg_type == "shutdown":
                logger.info("[Orchestrator] Received shutdown signal")
                self._shutdown_event.set()
                self._shutdown_stages()
                break
            else:
                logger.warning(f"[Orchestrator] Unknown message type: {msg_type}")

    async def _output_handler(self) -> None:
        """Poll all stages, handle transfers, send final outputs to main."""
        try:
            await self._output_handler_loop()
        except asyncio.CancelledError:
            logger.debug("[Orchestrator] _output_handler cancelled")
            return

    async def _output_handler_loop(self) -> None:
        """Inner loop for _output_handler (separated for clean cancellation).

        Control flow: poll raw → process through output processor → route.
        """
        while not self._shutdown_event.is_set():
            idle = True
            for stage_id in range(self.num_stages):
                if self._shutdown_event.is_set():
                    return

                # 1) Poll raw outputs from the stage
                try:
                    raw_outputs = await asyncio.wait_for(
                        self._poll_stage_raw(stage_id), timeout=0.001
                    )
                except asyncio.TimeoutError:
                    continue
                except asyncio.CancelledError:
                    raise
                except Exception:
                    if self._shutdown_event.is_set():
                        return
                    logger.exception(
                        "[Orchestrator] _poll_stage_raw failed for stage-%s",
                        stage_id,
                    )
                    raise

                if raw_outputs is None:
                    continue
                idle = False

                # 2) Process raw outputs through the output processor
                request_outputs = await self._process_stage_outputs(
                    stage_id, raw_outputs
                )

                # 3) Route each processed output
                stage_client = self.stage_clients[stage_id]
                for output in request_outputs:
                    req_id = output.request_id
                    finished = getattr(output, "finished", False)
                    req_state = self.request_states.get(req_id)
                    if req_state is None:
                        logger.warning(
                            "[Orchestrator] Dropping output for unknown req %s "
                            "at stage-%s (known reqs: %s)",
                            req_id,
                            stage_id,
                            list(self.request_states.keys()),
                        )
                        continue

                    # Send to main thread if this stage produces final output
                    if stage_client.final_output:
                        is_fully_done = (
                            finished
                            and stage_id == req_state.final_stage_id
                        )
                        await self.output_queue.put({
                            "type": "output",
                            "request_id": req_id,
                            "stage_id": stage_id,
                            "engine_outputs": [output],
                            "metrics": None,
                            "finished": is_fully_done,
                        })

                    # Forward to next stage if this stage finished
                    if finished and stage_id < req_state.final_stage_id:
                        await self._forward_to_next_stage(
                            req_id, stage_id, output, req_state
                        )

                    # Cleanup when the final stage is done
                    if finished and stage_id == req_state.final_stage_id:
                        self.request_states.pop(req_id, None)

            if idle:
                await asyncio.sleep(0.001)
            else:
                await asyncio.sleep(0)

    async def _forward_to_next_stage(
        self,
        req_id: str,
        stage_id: int,
        output: Any,
        req_state: _OrchestratorRequestState,
    ) -> None:
        """Forward output from current stage to the next stage.

        Handles the full pipeline: set outputs on current stage, compute
        next-stage inputs, build lightweight requests, and submit them.
        """
        if not isinstance(output, list):
            engine_outputs = [output]
        else:
            engine_outputs = output

        self.stage_clients[stage_id].set_engine_outputs(engine_outputs)

        next_stage_id = stage_id + 1
        next_client = self.stage_clients[next_stage_id]
        params = req_state.sampling_params_list[next_stage_id]

        # Process inputs for next stage
        try:
            next_inputs = next_client.process_engine_inputs(
                stage_list=self.stage_clients,
                prompt=req_state.prompt,
            )
        except Exception:
            logger.exception(
                "[Orchestrator] req=%s process_engine_inputs FAILED "
                "for stage-%s",
                req_id, next_stage_id,
            )
            raise

        # Build and submit requests for each input
        for next_input in next_inputs:
            # Check connector
            # TODO: connector is disabled in this implementation.
            if self.connectors:
                connector_key = (str(stage_id), str(next_stage_id))
                connector = self.connectors.get(connector_key)
                if connector:
                    logger.debug(
                        "[Orchestrator] Stage %s using connector from stage %s",
                        next_stage_id, stage_id,
                    )

            request = _build_engine_core_request_from_tokens(
                request_id=req_id,
                prompt=next_input,
                params=params,
                model_config=self.stage_vllm_configs[next_stage_id].model_config,
            )

            OmniInputProcessor.assign_request_id(request)
            request.request_id = req_id

            self.output_processors[next_stage_id].add_request(
                request=request,
                prompt=None,
                parent_req=None,
                request_index=0,
                queue=None,
            )

            await next_client.add_request_async(request)

    async def _poll_stage_raw(self, stage_id: int) -> Any | None:
        """Pull raw EngineCoreOutputs from a stage client without processing.

        Returns the raw outputs object (with .outputs, .timestamp,
        .scheduler_stats), or None when there is nothing to consume.
        """
        outputs = await self.stage_clients[stage_id].get_output_async()
        if not outputs.outputs:
            return None
        return outputs

    async def _process_stage_outputs(
        self, stage_id: int, raw_outputs: Any
    ) -> list[Any]:
        """Run the output processor on raw outputs, returning RequestOutputs.

        Also handles abort forwarding and scheduler stats updates.
        """
        processor = self.output_processors[stage_id]

        processed = processor.process_outputs(
            raw_outputs.outputs,
            getattr(raw_outputs, "timestamp", None),
            None,
        )

        if getattr(processed, "reqs_to_abort", None):
            await self.stage_clients[stage_id].abort_requests_async(
                processed.reqs_to_abort
            )

        try:
            if hasattr(raw_outputs, "scheduler_stats"):
                processor.update_scheduler_stats(raw_outputs.scheduler_stats)
        except Exception:
            pass

        return list(getattr(processed, "request_outputs", []) or [])

    async def _handle_add_request(self, msg: dict[str, Any]) -> None:
        """Handle an add_request message from the main thread."""
        stage_id = msg["stage_id"]
        request_id = msg["request_id"]
        prompt = msg["prompt"]
        original_prompt = msg.get("original_prompt", prompt)
        params = msg["params"]
        sampling_params_list = msg["sampling_params_list"]
        final_stage_id = msg["final_stage_id"]

        logger.info(
            "[Orchestrator] _handle_add_request: stage=%s req=%s "
            "prompt_type=%s original_prompt_type=%s final_stage=%s "
            "num_sampling_params=%d",
            stage_id, request_id, type(prompt).__name__,
            type(original_prompt).__name__,
            final_stage_id, len(sampling_params_list),
        )

        # Track request state — use original_prompt so downstream stages
        # (e.g. thinker2talker) can access the raw dict with multi_modal_data.
        self.request_states[request_id] = _OrchestratorRequestState(
            request_id=request_id,
            prompt=original_prompt,
            sampling_params_list=sampling_params_list,
            final_stage_id=final_stage_id,
        )

        # Stage-0 prompt is already a fully-formed OmniEngineCoreRequest
        # (pre-processed by AsyncOmniEngine.add_request, output processor
        # already registered there) — submit directly.
        request = prompt
        logger.info(
            "[Orchestrator] _handle_add_request: stage-%s req=%s "
            "submitting pre-processed EngineCoreRequest (token_ids=%d)",
            stage_id, request_id, len(request.prompt_token_ids),
        )
        await self.stage_clients[stage_id].add_request_async(request)

    async def _handle_abort(self, msg: dict[str, Any]) -> None:
        """Handle an abort message from the main thread."""
        request_ids = msg["request_ids"]
        for stage_id in range(self.num_stages):
            await self.stage_clients[stage_id].abort_requests_async(request_ids)
        for req_id in request_ids:
            self.request_states.pop(req_id, None)
        logger.info("[Orchestrator] Aborted request(s) %s", request_ids)

    def _shutdown_stages(self) -> None:
        """Shutdown all stage clients."""
        logger.info("[Orchestrator] Shutting down all stages")
        for stage_id, stage_client in enumerate(self.stage_clients):
            try:
                stage_client.shutdown()
                logger.info(f"[Orchestrator] Stage {stage_id} shut down")
            except Exception as e:
                logger.warning(
                    f"[Orchestrator] Failed to shutdown stage {stage_id}: {e}"
                )


def _run_orchestrator_in_thread(
    model: str,
    stage_configs: Any,
    request_queue: asyncio.Queue,
    output_queue: asyncio.Queue,
    stage_init_timeout: int,
    log_requests: bool,
    loop_ready: threading.Event,
    loop_holder: list,
) -> None:
    """Top-level entry point for the Orchestrator background thread.

    Creates a fresh asyncio event loop and runs the _Orchestrator.
    Exposes the loop via *loop_holder* so the main thread can schedule
    work on it (e.g. queue.put via call_soon_threadsafe).
    """
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    loop_holder.append(loop)
    loop_ready.set()

    try:
        orchestrator = _Orchestrator(
            model=model,
            stage_configs=stage_configs,
            request_queue=request_queue,
            output_queue=output_queue,
            stage_init_timeout=stage_init_timeout,
            log_requests=log_requests,
        )
        loop.run_until_complete(orchestrator.run())
    except Exception:
        logger.exception("[Orchestrator] Thread crashed")
        # Signal the main thread so it doesn't hang waiting for ready
        try:
            loop.run_until_complete(
                output_queue.put({"type": "error", "error": "Orchestrator thread crashed"})
            )
        except Exception:
            pass
        raise
    finally:
        loop.close()


# ============================================================
# AsyncOmniEngine — thin proxy in the caller's thread
# ============================================================


class AsyncOmniEngine:
    """Thin proxy that launches an Orchestrator in a background thread.

    All stage clients, input/output processors, and stage-to-stage transfer
    logic live inside the Orchestrator coroutine (running in its own thread
    with a dedicated asyncio event loop).  This class communicates with it
    via asyncio.Queues.

    Args:
        model: Model name or path
        stage_configs: List of stage configurations. If None, loads from model.
        stage_configs_path: Path to YAML file with stage configs.
        stage_init_timeout: Timeout for stage initialization (seconds)
        log_requests: Whether to log requests
        **kwargs: Additional arguments
    """

    def __init__(
        self,
        model: str,
        stage_configs: list[Any] | None = None,
        stage_configs_path: str | None = None,
        stage_init_timeout: int = 300,
        log_requests: bool = True,
        **kwargs: Any,
    ) -> None:
        self.model = model
        self.stage_init_timeout = stage_init_timeout
        self.log_requests = log_requests

        logger.info(f"[AsyncOmniEngine] Initializing with model {model}")
        logger.info(f"[AsyncOmniEngine] stage_configs: {stage_configs}")
        logger.info(f"[AsyncOmniEngine] stage_configs_path: {stage_configs_path}")
        logger.info(f"[AsyncOmniEngine] stage_init_timeout: {stage_init_timeout}")
        logger.info(f"[AsyncOmniEngine] log_requests: {log_requests}")
        logger.info(f"[AsyncOmniEngine] kwargs: {kwargs}")

        # --- Resolve stage configs (same logic as before) ---
        tokenizer = kwargs.get("tokenizer", None)
        base_engine_args = (
            {"tokenizer": tokenizer} if tokenizer is not None else None
        )

        if stage_configs_path is None:
            self.config_path = resolve_model_config_path(model)
            resolved_configs = load_stage_configs_from_model(
                model, base_engine_args=base_engine_args
            )
            if not resolved_configs:
                default_stage_cfg = self._create_default_diffusion_stage_cfg(
                    kwargs
                )
                resolved_configs = OmegaConf.create(default_stage_cfg)
        else:
            self.config_path = stage_configs_path
            resolved_configs = load_stage_configs_from_yaml(
                stage_configs_path, base_engine_args=base_engine_args
            )

        self.stage_configs = resolved_configs
        self.num_stages = len(self.stage_configs)

        logger.info(
            f"[AsyncOmniEngine] Launching Orchestrator thread with "
            f"{self.num_stages} stages"
        )

        # Create asyncio queues (will be used from the orchestrator's loop)
        self.request_queue: asyncio.Queue = asyncio.Queue()
        self.output_queue: asyncio.Queue = asyncio.Queue()

        # Launch orchestrator background thread
        loop_ready = threading.Event()
        self._orch_loop_holder: list[asyncio.AbstractEventLoop] = []

        self.orchestrator_thread = threading.Thread(
            target=_run_orchestrator_in_thread,
            args=(
                model,
                self.stage_configs,
                self.request_queue,
                self.output_queue,
                stage_init_timeout,
                log_requests,
                loop_ready,
                self._orch_loop_holder,
            ),
            daemon=True,
            name="orchestrator",
        )
        self.orchestrator_thread.start()

        # Wait for the orchestrator's event loop to be available
        loop_ready.wait()
        self._orch_loop: asyncio.AbstractEventLoop = self._orch_loop_holder[0]

        # Wait for ready signal from orchestrator (blocking, runs in this thread)
        logger.info("[AsyncOmniEngine] Waiting for Orchestrator ready signal")
        ready_msg = self._blocking_get_output(timeout=stage_init_timeout)
        if ready_msg is None:
            raise TimeoutError(
                f"Orchestrator did not become ready within "
                f"{stage_init_timeout}s"
            )

        if ready_msg.get("type") == "error":
            raise RuntimeError(
                f"Orchestrator failed to start: {ready_msg.get('error')}"
            )

        assert ready_msg["type"] == "ready", (
            f"Expected ready message, got {ready_msg['type']}"
        )

        self.num_stages = ready_msg["num_stages"]
        self.default_sampling_params_list = ready_msg[
            "default_sampling_params_list"
        ]
        self.stage_metadata = ready_msg["stage_metadata"]
        self.input_processor: OmniInputProcessor = ready_msg["input_processor"]
        self.output_processors: list[MultimodalOutputProcessor] = ready_msg[
            "output_processors"
        ]

        logger.info(
            f"[AsyncOmniEngine] Orchestrator ready with {self.num_stages} stages"
        )

    # ---- helpers for cross-thread queue access ----

    def _put_to_request_queue(self, msg: dict[str, Any]) -> None:
        """Thread-safe put onto the orchestrator's request_queue."""
        self._orch_loop.call_soon_threadsafe(
            self.request_queue.put_nowait, msg
        )

    def _blocking_get_output(self, timeout: float) -> dict[str, Any] | None:
        """Blocking get from the output_queue (used during init)."""
        fut = asyncio.run_coroutine_threadsafe(
            self.output_queue.get(), self._orch_loop
        )
        try:
            return fut.result(timeout=timeout)
        except concurrent.futures.TimeoutError:
            return None

    @staticmethod
    def _create_default_diffusion_stage_cfg(kwargs: dict[str, Any]) -> list:
        """Create a default single-stage diffusion config from kwargs."""
        return [
            {
                "stage_id": 0,
                "stage_type": "diffusion",
                "engine_args": kwargs,
                "final_output": True,
                "final_output_type": "image",
            }
        ]

    # ==================== Public API ====================

    async def add_request(
        self,
        stage_id: int,
        request_id: str,
        prompt: EngineCoreRequest | PromptType,
        params: SamplingParams | PoolingParams,
        sampling_params_list: list[Any] | None = None,
        final_stage_id: int = 0,
        arrival_time: float | None = None,
        lora_request: LoRARequest | None = None,
        tokenization_kwargs: dict[str, Any] | None = None,
        trace_headers: dict[str, str] | None = None,
        priority: int = 0,
        prompt_text: str | None = None,
    ) -> None:
        """Process stage 0 input locally, then send to the Orchestrator.

        For stage 0 requests (the common path), input processing and output
        processor registration happen here in the caller's thread, avoiding
        a queue + coroutine-switch round-trip.  The Orchestrator receives a
        ready-to-submit OmniEngineCoreRequest.
        """
        # Keep the original prompt for downstream stages (they need the raw
        # dict, e.g. for multi_modal_data).
        original_prompt = prompt

        if stage_id == 0 and not isinstance(prompt, EngineCoreRequest):
            # Inject global_request_id into the raw prompt
            if isinstance(prompt, dict):
                _inject_global_id(prompt, request_id)
            elif isinstance(prompt, list):
                for item in prompt:
                    _inject_global_id(item, request_id)

            # Full input processing (tokenization, multimodal, etc.)
            request = self.input_processor.process_inputs(
                request_id=request_id,
                prompt=prompt,
                params=params,
                arrival_time=arrival_time,
            )
            OmniInputProcessor.assign_request_id(request)
            request.request_id = request_id

            # Register with stage 0's output processor
            self.output_processors[0].add_request(
                request=request,
                prompt=prompt,
                parent_req=None,
                request_index=0,
                queue=None,
            )

            prompt = request

        self._put_to_request_queue({
            "type": "add_request",
            "stage_id": stage_id,
            "request_id": request_id,
            "prompt": prompt,
            "original_prompt": original_prompt,
            "params": params,
            "sampling_params_list": sampling_params_list or [],
            "final_stage_id": final_stage_id,
        })

    def try_get_output(self) -> dict[str, Any] | None:
        """Non-blocking read from the Orchestrator output_queue."""
        try:
            fut = asyncio.run_coroutine_threadsafe(
                asyncio.wait_for(self.output_queue.get(), timeout=0),
                self._orch_loop,
            )
            return fut.result(timeout=0.05)
        except Exception:
            return None

    def try_get_output_blocking(self, timeout: float = 0.05) -> dict[str, Any] | None:
        """Blocking read from the Orchestrator output_queue with timeout."""
        try:
            fut = asyncio.run_coroutine_threadsafe(
                asyncio.wait_for(self.output_queue.get(), timeout=timeout),
                self._orch_loop,
            )
            return fut.result(timeout=timeout + 0.1)
        except Exception:
            return None

    def get_stage_metadata(self, stage_id: int) -> dict[str, Any]:
        """Get cached metadata for a stage."""
        return self.stage_metadata[stage_id]

    async def abort(self, request_ids: list[str]) -> None:
        """Send abort message to the Orchestrator."""
        self._put_to_request_queue({
            "type": "abort",
            "request_ids": request_ids,
        })

    def shutdown(self) -> None:
        """Send shutdown message and wait for the Orchestrator thread to exit."""
        logger.info("[AsyncOmniEngine] Shutting down Orchestrator")
        try:
            self._put_to_request_queue({"type": "shutdown"})
        except Exception:
            pass
        if hasattr(self, "orchestrator_thread") and self.orchestrator_thread.is_alive():
            self.orchestrator_thread.join(timeout=10)
            if self.orchestrator_thread.is_alive():
                logger.warning(
                    "[AsyncOmniEngine] Orchestrator thread did not exit in time"
                )

    def __del__(self) -> None:
        try:
            self.shutdown()
        except Exception:
            pass
